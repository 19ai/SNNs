{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tutorial on self-normalizing networks on the MNIST data set: convolutional neural networks\n",
    "\n",
    "*Author:* Guenter Klambauer, 2017\n",
    "\n",
    "tested under Python 3.5 and Tensorflow 1.1\n",
    "\n",
    "Derived from: [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import numbers\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.layers import utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import erf,erfc\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Definition of scaled exponential linear units (SELUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    with ops.name_scope('elu') as scope:\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Definition of dropout variant for SNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_selu(x, rate, alpha= -myAlpha*myLambda, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        # Do nothing if we know keep_prob == 1\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        # uniform [keep_prob, 1.0 + keep_prob)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        #binary_tensor2 = math_ops.ceil(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        #a = tf.sqrt(1.0/(keep_prob+alpha^2*keep_prob*(1.0-keep_prob)))\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "        #a = tf.sqrt(tf.div(1.0, tf.add(keep_prob ,tf.multiply(tf.pow(alpha,2) , tf.multiply(keep_prob,    tf.subtract(1.0,keep_prob)))) ))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        #b = tf.neg( tf.mul(a , (tf.multiply(tf.subtract(1.0,keep_prob),alpha))))\n",
    "        ret = a * ret + b\n",
    "        #ret = tf.add(tf.multiply(a , ret) , b)\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Scale input to zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.025\n",
    "training_iters = 50\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "keep_prob_ReLU = 0.75 # Dropout, probability to keep units\n",
    "dropout_prob_SNN = 0.05 # Dropout, probability to dropout units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability for ReLU)\n",
    "dropout_prob =  tf.placeholder(tf.float32) #dropout (dropout probability for SNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def conv2d_SNN(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return selu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net_ReLU(x, weights, biases, keep_prob):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net_SNN(x, weights, biases, dropout_prob):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d_SNN(x, weights['wc1'], biases['bc1'],)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d_SNN(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = selu(fc1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    fc1 = dropout_selu(fc1, dropout_prob)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RELU: Store layers weight & bias\n",
    "## Improved with MSRA initialization\n",
    "\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32],stddev=np.sqrt(2/25)) ),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64],stddev=np.sqrt(2/(25*32)))),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024],stddev=np.sqrt(2/(7*7*64)))),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes],stddev=np.sqrt(2/(1024))))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32],stddev=0)),\n",
    "    'bc2': tf.Variable(tf.random_normal([64],stddev=0)),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024],stddev=0)),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes],stddev=0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Initialization with STDDEV of sqrt(1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SNN: Store layers weight & bias\n",
    "weights2 = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32],stddev=np.sqrt(1/25)) ),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64],stddev=np.sqrt(1/(25*32)))),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024],stddev=np.sqrt(1/(7*7*64)))),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes],stddev=np.sqrt(1/(1024))))\n",
    "}\n",
    "\n",
    "biases2 = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32],stddev=0)),\n",
    "    'bc2': tf.Variable(tf.random_normal([64],stddev=0)),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024],stddev=0)),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes],stddev=0))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred_ReLU = conv_net_ReLU(x, weights, biases, keep_prob)\n",
    "pred_SNN = conv_net_SNN(x, weights2, biases2, dropout_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost_ReLU = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_ReLU, labels=y))\n",
    "cost_SNN = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_SNN, labels=y))\n",
    "\n",
    "optimizer_ReLU = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_ReLU)\n",
    "optimizer_SNN = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_SNN)\n",
    "\n",
    "# Evaluate ReLU model\n",
    "correct_pred_ReLU = tf.equal(tf.argmax(pred_ReLU, 1), tf.argmax(y, 1))\n",
    "accuracy_ReLU = tf.reduce_mean(tf.cast(correct_pred_ReLU, tf.float32))\n",
    "\n",
    "# Evaluate SNN model\n",
    "correct_pred_SNN = tf.equal(tf.argmax(pred_SNN, 1), tf.argmax(y, 1))\n",
    "accuracy_SNN = tf.reduce_mean(tf.cast(correct_pred_SNN, tf.float32))\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loss_protocol_ReLU = []\n",
    "training_loss_protocol_SNN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELU: Nbr of updates: 1, Minibatch Loss= 2.474862, Training Accuracy= 0.21875\n",
      "SNN: Nbr of updates: 1, Minibatch Loss= 3.402813, Training Accuracy= 0.42188\n",
      "RELU: Nbr of updates: 2, Minibatch Loss= 2.084900, Training Accuracy= 0.29688\n",
      "SNN: Nbr of updates: 2, Minibatch Loss= 3.249937, Training Accuracy= 0.54688\n",
      "RELU: Nbr of updates: 3, Minibatch Loss= 1.876246, Training Accuracy= 0.40625\n",
      "SNN: Nbr of updates: 3, Minibatch Loss= 1.049787, Training Accuracy= 0.67188\n",
      "RELU: Nbr of updates: 4, Minibatch Loss= 1.623726, Training Accuracy= 0.60938\n",
      "SNN: Nbr of updates: 4, Minibatch Loss= 0.688420, Training Accuracy= 0.78125\n",
      "RELU: Nbr of updates: 5, Minibatch Loss= 1.503145, Training Accuracy= 0.57031\n",
      "SNN: Nbr of updates: 5, Minibatch Loss= 0.512824, Training Accuracy= 0.89844\n",
      "RELU: Nbr of updates: 6, Minibatch Loss= 1.513853, Training Accuracy= 0.66406\n",
      "SNN: Nbr of updates: 6, Minibatch Loss= 0.511910, Training Accuracy= 0.89844\n",
      "RELU: Nbr of updates: 7, Minibatch Loss= 1.359246, Training Accuracy= 0.63281\n",
      "SNN: Nbr of updates: 7, Minibatch Loss= 0.389784, Training Accuracy= 0.89844\n",
      "RELU: Nbr of updates: 8, Minibatch Loss= 1.252284, Training Accuracy= 0.70312\n",
      "SNN: Nbr of updates: 8, Minibatch Loss= 0.369953, Training Accuracy= 0.88281\n",
      "RELU: Nbr of updates: 9, Minibatch Loss= 1.193034, Training Accuracy= 0.67969\n",
      "SNN: Nbr of updates: 9, Minibatch Loss= 0.313167, Training Accuracy= 0.94531\n",
      "RELU: Nbr of updates: 10, Minibatch Loss= 1.102128, Training Accuracy= 0.75000\n",
      "SNN: Nbr of updates: 10, Minibatch Loss= 0.330352, Training Accuracy= 0.90625\n",
      "RELU: Nbr of updates: 11, Minibatch Loss= 1.141619, Training Accuracy= 0.68750\n",
      "SNN: Nbr of updates: 11, Minibatch Loss= 0.362560, Training Accuracy= 0.93750\n",
      "RELU: Nbr of updates: 12, Minibatch Loss= 0.983428, Training Accuracy= 0.79688\n",
      "SNN: Nbr of updates: 12, Minibatch Loss= 0.282479, Training Accuracy= 0.94531\n",
      "RELU: Nbr of updates: 13, Minibatch Loss= 0.992611, Training Accuracy= 0.78125\n",
      "SNN: Nbr of updates: 13, Minibatch Loss= 0.339259, Training Accuracy= 0.92188\n",
      "RELU: Nbr of updates: 14, Minibatch Loss= 0.934061, Training Accuracy= 0.81250\n",
      "SNN: Nbr of updates: 14, Minibatch Loss= 0.235764, Training Accuracy= 0.96094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5563eb2fdc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                        keep_prob: keep_prob_ReLU})\n\u001b[1;32m     13\u001b[0m         sess.run(optimizer_SNN, feed_dict={x: batch_x_norm, y: batch_y,\n\u001b[0;32m---> 14\u001b[0;31m                                        dropout_prob: dropout_prob_SNN})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/virtualenvs/tf-alpha/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/virtualenvs/tf-alpha/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/virtualenvs/tf-alpha/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Dropbox/virtualenvs/tf-alpha/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/virtualenvs/tf-alpha/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    # Keep training until reach max iterations\n",
    "    while step < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x_norm = scaler.transform(batch_x)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer_ReLU, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: keep_prob_ReLU})\n",
    "        sess.run(optimizer_SNN, feed_dict={x: batch_x_norm, y: batch_y,\n",
    "                                       dropout_prob: dropout_prob_SNN})\n",
    "        \n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            #batch_x, batch_y = mnist.test.next_batch(batch_size)\n",
    "            #batch_x_norm = scaler.transform(batch_x)\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss_ReLU, acc_ReLU = sess.run([cost_ReLU, accuracy_ReLU], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.0})\n",
    "            training_loss_protocol_ReLU.append(loss_ReLU)\n",
    "            \n",
    "            loss_SNN, acc_SNN = sess.run([cost_SNN, accuracy_SNN], feed_dict={x: batch_x_norm,\n",
    "                                                              y: batch_y,\n",
    "                                                              dropout_prob: 0.0})\n",
    "            training_loss_protocol_SNN.append(loss_SNN)\n",
    "            \n",
    "            print( \"RELU: Nbr of updates: \" + str(step+1) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_ReLU) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc_ReLU))\n",
    "            \n",
    "            print( \"SNN: Nbr of updates: \" + str(step+1) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_SNN) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc_SNN))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\\n\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"ReLU: Testing Accuracy:\", sess.run(accuracy_ReLU, feed_dict={x: mnist.test.images[:512],\n",
    "                                      y: mnist.test.labels[:512],\n",
    "                                      keep_prob: 1.0}))\n",
    "    print(\"SNN: Testing Accuracy:\", sess.run(accuracy_SNN, feed_dict={x: scaler.transform(mnist.test.images[:512]),\n",
    "                                      y: mnist.test.labels[:512],\n",
    "                                      dropout_prob: 0.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( training_loss_protocol_ReLU, label='Loss ReLU-CNN')\n",
    "ax.plot( training_loss_protocol_SNN, label='Loss SNN')\n",
    "ax.set_yscale('log')  # log scale\n",
    "ax.set_xlabel('iterations/updates')\n",
    "ax.set_ylabel('training loss')\n",
    "fig.tight_layout()\n",
    "ax.legend()\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-alpha",
   "language": "python",
   "name": "tf-alpha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
